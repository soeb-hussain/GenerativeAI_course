{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Name - Soeb Hussain\n",
        "# Nuid - 002747200\n",
        "# Assignment 2\n",
        "# CSYE7340 - GenAI"
      ],
      "metadata": {
        "id": "OqQ1-aiJ_9-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2)\n",
        "Find the similarity between two sentences or paragraphs. This assignment will help you to become familiar with NLP functionalities such as tokenization, stemming, and word embeddings."
      ],
      "metadata": {
        "id": "RPismleMZ_Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = [\"The quick brown fox jumps over the lazy dog. The sun is shining brightly in the clear blue sky.\",\n",
        "              \"A lazy dog is jumped over by a quick brown fox. The sky is clear, and the sun shines brightly.\"]"
      ],
      "metadata": {
        "id": "m8iP8vW0e-kR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Preprocess and tokenize your paragraphs, store them in a list called 'paragraphs'\n",
        "\n",
        "# Initialize a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Create TF-IDF vectors for the paragraphs\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(paragraphs)\n",
        "\n",
        "# Calculate cosine similarity between paragraphs\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "print(f\" Paragraph similarity score : {similarity_matrix[0][1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJidiE5xfe7t",
        "outputId": "d522c44a-1911-47ea-e847-dd71055f8c2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Paragraph similarity score : 0.6651934954020767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8TuVF-cZ4Wx",
        "outputId": "455ad26d-e14f-4434-f61f-3361f396985b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between the paragraphs: 0.76\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input paragraphs\n",
        "paragraph1 = paragraphs[0]\n",
        "paragraph2 = paragraphs[1]\n",
        "\n",
        "# Tokenize and vectorize paragraphs\n",
        "doc1 = nlp(paragraph1)\n",
        "doc2 = nlp(paragraph2)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "vector1 = doc1.vector.reshape(1, -1)\n",
        "vector2 = doc2.vector.reshape(1, -1)\n",
        "similarity = cosine_similarity(vector1, vector2)[0][0]\n",
        "\n",
        "# Print the similarity\n",
        "print(f\"Similarity between the paragraphs: {similarity:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list all the pretrained models\n",
        "# import gensim.downloader as api\n",
        "# api.info()"
      ],
      "metadata": {
        "id": "VWHPF3_OjJA8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Load a pre-trained Word2Vec model\n",
        "w2v_model = api.load(\"word2vec-google-news-300\")\n",
        "# w2v_model = api.load(\"20-newsgroups\")\n",
        "\n",
        "# Calculate paragraph embeddings\n",
        "paragraph_embeddings = []\n",
        "for paragraph in paragraphs:\n",
        "    words = paragraph.split()\n",
        "    valid_words = [word for word in words if word in w2v_model.key_to_index]\n",
        "    if valid_words:\n",
        "        paragraph_vector = np.mean([w2v_model[word] for word in valid_words], axis=0)\n",
        "        paragraph_embeddings.append(paragraph_vector)\n",
        "\n",
        "# Calculate cosine similarity between paragraph embeddings\n",
        "similarity_matrix = cosine_similarity(paragraph_embeddings)\n",
        "\n",
        "# Identify similairity score\n",
        "print('similairy score between two paragraph is ',similarity_matrix[1][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNCsX459bIVN",
        "outputId": "1d9bd99d-bcee-49e6-a0fc-ef5f3764998d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similairy score between two paragraph is  0.8790373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "id": "hougMwUGlKPF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Calculate BERT embeddings for paragraphs\n",
        "paragraph_embeddings = []\n",
        "for paragraph in paragraphs:\n",
        "    input_ids = tokenizer(paragraph, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**input_ids)\n",
        "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "    paragraph_embeddings.append(embeddings)\n",
        "\n",
        "# Convert embeddings to numpy arrays\n",
        "paragraph_embeddings = [embedding.numpy() for embedding in paragraph_embeddings]\n",
        "\n",
        "# Calculate cosine similarity between paragraph embeddings\n",
        "similarity_matrix = cosine_similarity((paragraph_embeddings[0]).reshape(1, -1),(paragraph_embeddings[1]).reshape(1, -1))\n",
        "\n",
        "print('similairy score between two paragraph is ',similarity_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsDY0XV0eqM4",
        "outputId": "47ad2877-16ae-47b1-f1d6-34404519bc56"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similairy score between two paragraph is  [[0.9220655]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9RcNP0nmho_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}