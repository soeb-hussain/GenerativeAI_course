{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hougMwUGlKPF",
    "outputId": "dfee6024-fb58-4de2-a60b-c3b5d3f183a9"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqQ1-aiJ_9-1"
   },
   "source": [
    "# Name - Soeb Hussain\n",
    "# Nuid - 002747200\n",
    "# Assignment 2\n",
    "# CSYE7340 - GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPismleMZ_Vg"
   },
   "source": [
    "# 2)\n",
    "Find the similarity between two sentences or paragraphs. This assignment will help you to become familiar with NLP functionalities such as tokenization, stemming, and word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "m8iP8vW0e-kR"
   },
   "outputs": [],
   "source": [
    "paragraphs = [\"The quick brown fox jumps over the lazy dog. The sun is shining brightly in the clear blue sky.\",\n",
    "              \"A lazy dog is jumped over by a quick brown fox. The sky is clear, and the sun shines brightly.\"]\n",
    "\n",
    "paragraphs2 =[\"The forest was sparsely populated with trees.\",\"The woodland had only a sparse number of trees.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJidiE5xfe7t",
    "outputId": "4eca5a9e-9cb2-486c-f980-9eaf9f7e56cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paragraph similarity score : 0.9789419964915469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Create TF-IDF vectors for the paragraphs\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(paragraphs)\n",
    "\n",
    "# Calculate cosine similarity between paragraphs\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print(f\" Paragraph similarity score : {similarity_matrix[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0Jv8Z2dfOsN",
    "outputId": "fb35a197-fab2-450f-fb93-c46de974a6f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paragraph similarity score : 0.15592892548708362\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer2 = TfidfVectorizer()\n",
    "\n",
    "# Create TF-IDF vectors for the paragraphs\n",
    "tfidf_matrix2 = tfidf_vectorizer2.fit_transform(paragraphs2)\n",
    "\n",
    "# Calculate cosine similarity between paragraphs\n",
    "similarity_matrix2 = cosine_similarity(tfidf_matrix2, tfidf_matrix2)\n",
    "\n",
    "print(f\" Paragraph similarity score : {similarity_matrix2[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1BqhRSxawLG",
    "outputId": "81b75229-7939-4422-9653-61b193935851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paragraph similarity score : 0.9789419964915469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [' ', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Initialize the Porter Stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Apply stemming to each token\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "\n",
    "# Initialize a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english')\n",
    "\n",
    "# Create TF-IDF vectors for the paragraphs\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(paragraphs)\n",
    "\n",
    "# Calculate cosine similarity between paragraphs\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "print(f\" Paragraph similarity score : {similarity_matrix[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtevbG7pfmQy",
    "outputId": "fa216f8d-03a5-4f9a-b56e-28cbe1e5624a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paragraph similarity score : 0.824273815029136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize a TF-IDF vectorizer\n",
    "tfidf_vectorizer2 = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english')\n",
    "\n",
    "# Create TF-IDF vectors for the paragraphs\n",
    "tfidf_matrix2 = tfidf_vectorizer2.fit_transform(paragraphs2)\n",
    "\n",
    "# Calculate cosine similarity between paragraphs\n",
    "similarity_matrix2 = cosine_similarity(tfidf_matrix2, tfidf_matrix2)\n",
    "\n",
    "print(f\" Paragraph similarity score : {similarity_matrix2[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8TuVF-cZ4Wx",
    "outputId": "63c58073-3b44-4618-8667-14e30102b7a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the paragraphs: 0.76\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Input paragraphs\n",
    "paragraph1 = paragraphs[0]\n",
    "paragraph2 = paragraphs[1]\n",
    "\n",
    "# Tokenize and vectorize paragraphs\n",
    "doc1 = nlp(paragraph1)\n",
    "doc2 = nlp(paragraph2)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "vector1 = doc1.vector.reshape(1, -1)\n",
    "vector2 = doc2.vector.reshape(1, -1)\n",
    "similarity = cosine_similarity(vector1, vector2)[0][0]\n",
    "\n",
    "# Print the similarity\n",
    "print(f\"Similarity between the paragraphs: {similarity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6p_5yrAif66j",
    "outputId": "68c362cb-6cf2-4205-bc8e-f840d2647720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the paragraphs: 0.54\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paragraph12 = paragraphs2[0]\n",
    "paragraph22 = paragraphs2[1]\n",
    "\n",
    "# Tokenize and vectorize paragraphs\n",
    "doc12 = nlp(paragraph12)\n",
    "doc22 = nlp(paragraph22)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "vector12 = doc12.vector.reshape(1, -1)\n",
    "vector22 = doc22.vector.reshape(1, -1)\n",
    "similarity2 = cosine_similarity(vector12, vector22)[0][0]\n",
    "\n",
    "# Print the similarity\n",
    "print(f\"Similarity between the paragraphs: {similarity2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "VWHPF3_OjJA8"
   },
   "outputs": [],
   "source": [
    "# list all the pretrained models\n",
    "# import gensim.downloader as api\n",
    "# api.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vNCsX459bIVN",
    "outputId": "fdd935ab-efe4-4839-a1a0-f8af6c650010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similairy score between two paragraph is  0.8790373\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Load a pre-trained Word2Vec model\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "# w2v_model = api.load(\"20-newsgroups\")\n",
    "\n",
    "# Calculate paragraph embeddings\n",
    "paragraph_embeddings = []\n",
    "for paragraph in paragraphs:\n",
    "    words = paragraph.split()\n",
    "    valid_words = [word for word in words if word in w2v_model.key_to_index]\n",
    "    if valid_words:\n",
    "        paragraph_vector = np.mean([w2v_model[word] for word in valid_words], axis=0)\n",
    "        paragraph_embeddings.append(paragraph_vector)\n",
    "\n",
    "# Calculate cosine similarity between paragraph embeddings\n",
    "similarity_matrix = cosine_similarity(paragraph_embeddings)\n",
    "\n",
    "# Identify similairity score\n",
    "print('similairy score between two paragraph is ',similarity_matrix[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFcBXDCugH0w",
    "outputId": "a0520554-c661-4df3-8b44-3c63658ae583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similairy score between two paragraph is  0.6870287\n"
     ]
    }
   ],
   "source": [
    "paragraph_embeddings = []\n",
    "for paragraph in paragraphs2:\n",
    "    words = paragraph.split()\n",
    "    valid_words = [word for word in words if word in w2v_model.key_to_index]\n",
    "    if valid_words:\n",
    "        paragraph_vector = np.mean([w2v_model[word] for word in valid_words], axis=0)\n",
    "        paragraph_embeddings.append(paragraph_vector)\n",
    "\n",
    "# Calculate cosine similarity between paragraph embeddings\n",
    "similarity_matrix = cosine_similarity(paragraph_embeddings)\n",
    "\n",
    "# Identify similairity score\n",
    "print('similairy score between two paragraph is ',similarity_matrix[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IsDY0XV0eqM4",
    "outputId": "05f91ec8-ad34-4737-bd2e-bf8d579befaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similairy score between two paragraph is  [[0.9220655]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Calculate BERT embeddings for paragraphs\n",
    "paragraph_embeddings = []\n",
    "for paragraph in paragraphs:\n",
    "    input_ids = tokenizer(paragraph, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids)\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    paragraph_embeddings.append(embeddings)\n",
    "\n",
    "# Convert embeddings to numpy arrays\n",
    "paragraph_embeddings = [embedding.numpy() for embedding in paragraph_embeddings]\n",
    "\n",
    "# Calculate cosine similarity between paragraph embeddings\n",
    "similarity_matrix = cosine_similarity((paragraph_embeddings[0]).reshape(1, -1),(paragraph_embeddings[1]).reshape(1, -1))\n",
    "\n",
    "print('similairy score between two paragraph is ',similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZczZ7VP7gR5m",
    "outputId": "b79c2ba7-e082-4a38-978c-bbf10f3f2d17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similairy score between two paragraph is  [[0.91852266]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paragraph_embeddings = []\n",
    "for paragraph in paragraphs2:\n",
    "    input_ids = tokenizer(paragraph, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**input_ids)\n",
    "    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "    paragraph_embeddings.append(embeddings)\n",
    "\n",
    "# Convert embeddings to numpy arrays\n",
    "paragraph_embeddings = [embedding.numpy() for embedding in paragraph_embeddings]\n",
    "\n",
    "# Calculate cosine similarity between paragraph embeddings\n",
    "similarity_matrix = cosine_similarity((paragraph_embeddings[0]).reshape(1, -1),(paragraph_embeddings[1]).reshape(1, -1))\n",
    "\n",
    "print('similairy score between two paragraph is ',similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G9RcNP0nmho_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
